{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download datasets from bucket"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),  \n",
    "])\n",
    "\n",
    "train_data_path = 'data/train/casia-100'\n",
    "test_data_path = 'data/test/casia-100'\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_data = datasets.ImageFolder(train_data_path, transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_data = datasets.ImageFolder(test_data_path, transform=train_transform)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torch import nn, optim\n",
    "import os\n",
    "model = resnet18()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and validate the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions for training and validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_accuracy(model: nn.Module, train=False):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    n = 0\n",
    "    loader = train_loader if train else test_loader \n",
    "    predictions, label_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in iter(loader):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            model.eval()\n",
    "            output = model(imgs).data\n",
    "            pred = output.max(1)[1]\n",
    "            correct += pred.eq(labels.data).sum().item()\n",
    "            total += imgs.shape[0]\n",
    "            predictions.append(pred)\n",
    "            label_list.append(labels)\n",
    "            n += 1\n",
    "    return correct / total, predictions, label_list\n",
    "\n",
    "def plot_training_curve(iters, losses, epochs, train_acc, val_acc):\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.plot(iters, losses, label=\"Train\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    #save plot to file\n",
    "    time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    plt.savefig(\"figures/loss_curve_{}.png\".format(time))\n",
    "\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.plot(epochs, train_acc, label=\"Train\")\n",
    "    plt.plot(epochs, val_acc, label=\"Validation\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Training Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    plt.savefig(\"figures/learning_curve_{}.png\".format(time))\n",
    "\n",
    "def train(\n",
    "    learning_rate = 0.1,\n",
    "    num_epochs = 30,\n",
    "    weight_decay = 0.0,\n",
    "    momentum = 0.9,\n",
    "    output_to_file = True,\n",
    "    always_output = True, \n",
    "    scheduling = False,\n",
    "    lr_milestones = [15, 20, 25],\n",
    "    lr_gamma = 0.1,\n",
    "    eval = True,\n",
    "    data_loader = train_loader,\n",
    "    loss_output_mod = 10,\n",
    "    filename_prefix = 'model',\n",
    "    savestate = True,\n",
    "    ):\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SGD(\n",
    "        model.fc.parameters(),\n",
    "        lr=learning_rate,\n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay\n",
    "        )\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=lr_milestones, gamma=lr_gamma)\n",
    "\n",
    "    if output_to_file:\n",
    "        outputfile = open('output_'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'.txt', 'w')\n",
    "\n",
    "    def output(text):\n",
    "        if output_to_file:\n",
    "            outputfile.write(text + '\\n')\n",
    "        else:\n",
    "            print(text)\n",
    "\n",
    "    iters, losses, train_acc, val_acc, epochs = [], [], [], [], []\n",
    "\n",
    "    n = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_tic = time.perf_counter()\n",
    "        for imgs, labels in iter(data_loader):\n",
    "            tic = time.perf_counter()\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            out = model(imgs)\n",
    "            loss = loss_fn(out, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            toc = time.perf_counter()\n",
    "\n",
    "            if (n % loss_output_mod == 0) or always_output:\n",
    "                output('epoch: {}, iter: {}, loss: {}, time: {}'.format(epoch, n, loss, toc - tic))\n",
    "            if eval:\n",
    "                iters.append(n)\n",
    "                losses.append(float(loss)/batch_size) # compute *average* loss\n",
    "            n += 1\n",
    "        \n",
    "        epochs.append(epoch)\n",
    "\n",
    "        if eval: \n",
    "            curr_train_acc = get_accuracy(model, train=True)[0]\n",
    "            curr_val_acc = get_accuracy(model, train=False)[0]\n",
    "            train_acc.append(curr_train_acc) # compute training accuracy \n",
    "            val_acc.append(curr_val_acc)  # compute validation accuracy\n",
    "            output(\"Train accuracy: {}\".format(curr_train_acc))\n",
    "            output(\"Validation accuracy: {}\".format(curr_val_acc))\n",
    "        \n",
    "        if savestate:\n",
    "            torch.save(model.state_dict(), 'model-states/' + filename_prefix + '_epoch_' + str(epoch) + '.pt')\n",
    "        \n",
    "        if scheduling:\n",
    "            lr_scheduler.step() \n",
    "\n",
    "        epoch_toc = time.perf_counter()\n",
    "        output('epoch: {}, time: {}'.format(epoch, epoch_toc - epoch_tic))\n",
    "        if output_to_file:\n",
    "            outputfile.flush()\n",
    "\n",
    "    if eval: \n",
    "        output(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n",
    "        output(\"Final Validation Accuracy: {}\".format(val_acc[-1]))\n",
    "        plot_training_curve(iters, losses, epochs, train_acc, val_acc)\n",
    "    else: \n",
    "        torch.save(model.state_dict(), 'model-states/' + filename_prefix + '_final.pt')\n",
    "        output(\"Final Training Accuracy: {}\".format(get_accuracy(model, train=True)[0]))\n",
    "        # output(\"Final Validation Accuracy: {}\".format(get_accuracy(model, train=False)))\n",
    "    \n",
    "    if output_to_file:\n",
    "        outputfile.close()\n",
    "    \n",
    "    if eval:\n",
    "        return get_accuracy(model, train=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load digiface pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes_pretrain = 2000\n",
    "model.fc = nn.Linear(512, num_classes_pretrain)\n",
    "model.load_state_dict(torch.load('digiface_pretrained_v4_final.pt', map_location=torch.device('mps')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a last classifier layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.2, inplace=True), \n",
    "    torch.nn.Linear(in_features=512, \n",
    "                    out_features=100, # same number of output units as our number of classes\n",
    "                    bias=True)).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune digiface pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_training_curve() missing 1 required positional argument: 'val_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m\n\u001b[0;32m----> 5\u001b[0m precision1, predictions1, label_list1 \u001b[39m=\u001b[39m train(\n\u001b[1;32m      6\u001b[0m     \u001b[39meval\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      7\u001b[0m     data_loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m      8\u001b[0m     output_to_file\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      9\u001b[0m     always_output\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     10\u001b[0m     scheduling\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     11\u001b[0m     learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m     12\u001b[0m     num_epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     loss_output_mod\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     14\u001b[0m     savestate\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     15\u001b[0m )\n",
      "Cell \u001b[0;32mIn[10], line 132\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(learning_rate, num_epochs, weight_decay, momentum, output_to_file, always_output, scheduling, lr_milestones, lr_gamma, eval, data_loader, loss_output_mod, filename_prefix, savestate)\u001b[0m\n\u001b[1;32m    130\u001b[0m     output(\u001b[39m\"\u001b[39m\u001b[39mFinal Training Accuracy: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(train_acc[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\n\u001b[1;32m    131\u001b[0m     output(\u001b[39m\"\u001b[39m\u001b[39mFinal Validation Accuracy: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(val_acc[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]))\n\u001b[0;32m--> 132\u001b[0m     plot_training_curve(iters, losses, train_acc, val_acc)\n\u001b[1;32m    133\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[1;32m    134\u001b[0m     torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m'\u001b[39m\u001b[39mmodel-states/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m filename_prefix \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_final.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: plot_training_curve() missing 1 required positional argument: 'val_acc'"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "precision1, predictions1, label_list1 = train(\n",
    "    eval=True,\n",
    "    data_loader=train_loader,\n",
    "    output_to_file=True,\n",
    "    always_output=False,\n",
    "    scheduling=False,\n",
    "    learning_rate=learning_rate,\n",
    "    num_epochs=20,\n",
    "    loss_output_mod=10,\n",
    "    savestate=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load imagenet pretrained model\n",
    "freeze params\n",
    "add last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.2, inplace=True), \n",
    "    torch.nn.Linear(in_features=512, \n",
    "                    out_features=100, # same number of output units as our number of classes\n",
    "                    bias=True)).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune imagenet pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "precision2, predictions2, label_list2 = train(\n",
    "    eval=True,\n",
    "    data_loader=train_loader,\n",
    "    output_to_file=True,\n",
    "    always_output=False,\n",
    "    scheduling=False,\n",
    "    learning_rate=learning_rate,\n",
    "    num_epochs=20,\n",
    "    loss_output_mod=10,\n",
    "    savestate=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model with no pretrained weights\n",
    "freeze params\n",
    "add last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(p=0.2, inplace=True), \n",
    "    torch.nn.Linear(in_features=512, \n",
    "                    out_features=100, # same number of output units as our number of classes\n",
    "                    bias=True)).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune model with no pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "precision3, predictions3, label_list3 = train(\n",
    "    eval=True,\n",
    "    data_loader=train_loader,\n",
    "    output_to_file=True,\n",
    "    always_output=False,\n",
    "    scheduling=False,\n",
    "    learning_rate=learning_rate,\n",
    "    num_epochs=20,\n",
    "    loss_output_mod=10,\n",
    "    savestate=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
